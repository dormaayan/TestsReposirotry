/** 
 * Simple Job: Map -> Reduce -> Map -> Reduce. All functions preserve all fields (hence all properties). Increases parallelism between 1st reduce and 2nd map, so the hash partitioning from 1st reduce is not reusable. Expected to re-establish partitioning between reduce and map, via hash, because random is a full network transit as well.
 */
@Test public void checkPropertyHandlingWithIncreasingGlobalParallelism1(){
  final int p=DEFAULT_PARALLELISM;
  ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
  env.setParallelism(p);
  DataSet<Long> set1=env.generateSequence(0,1).setParallelism(p);
  set1.map(new IdentityMapper<Long>()).withForwardedFields("*").setParallelism(p).name("Map1").groupBy("*").reduceGroup(new IdentityGroupReducer<Long>()).withForwardedFields("*").setParallelism(p).name("Reduce1").map(new IdentityMapper<Long>()).withForwardedFields("*").setParallelism(p * 2).name("Map2").groupBy("*").reduceGroup(new IdentityGroupReducer<Long>()).withForwardedFields("*").setParallelism(p * 2).name("Reduce2").output(new DiscardingOutputFormat<Long>()).setParallelism(p * 2).name("Sink");
  Plan plan=env.createProgramPlan();
  OptimizedPlan oPlan=compileNoStats(plan);
  SinkPlanNode sinkNode=oPlan.getDataSinks().iterator().next();
  SingleInputPlanNode red2Node=(SingleInputPlanNode)sinkNode.getPredecessor();
  SingleInputPlanNode map2Node=(SingleInputPlanNode)red2Node.getPredecessor();
  ShipStrategyType mapIn=map2Node.getInput().getShipStrategy();
  ShipStrategyType redIn=red2Node.getInput().getShipStrategy();
  Assert.assertEquals("Invalid ship strategy for an operator.",ShipStrategyType.PARTITION_HASH,mapIn);
  Assert.assertEquals("Invalid ship strategy for an operator.",ShipStrategyType.FORWARD,redIn);
}
