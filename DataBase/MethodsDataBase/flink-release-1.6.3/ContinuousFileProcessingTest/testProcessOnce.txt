@Test public void testProcessOnce() throws Exception {
  String testBasePath=hdfsURI + "/" + UUID.randomUUID()+ "/";
  final OneShotLatch latch=new OneShotLatch();
  Tuple2<org.apache.hadoop.fs.Path,String> bootstrap=createFileAndFillWithData(testBasePath,"file",NO_OF_FILES + 1,"This is test line.");
  Assert.assertTrue(hdfs.exists(bootstrap.f0));
  final Set<String> filesToBeRead=new TreeSet<>();
  filesToBeRead.add(bootstrap.f0.getName());
  TextInputFormat format=new TextInputFormat(new Path(testBasePath));
  format.setFilesFilter(FilePathFilter.createDefaultFilter());
  final ContinuousFileMonitoringFunction<String> monitoringFunction=createTestContinuousFileMonitoringFunction(format,FileProcessingMode.PROCESS_ONCE);
  final FileVerifyingSourceContext context=new FileVerifyingSourceContext(latch,monitoringFunction);
  final Thread t=new Thread(){
    @Override public void run(){
      try {
        monitoringFunction.open(new Configuration());
        monitoringFunction.run(context);
        context.close();
      }
 catch (      Exception e) {
        Assert.fail(e.getMessage());
      }
    }
  }
;
  t.start();
  if (!latch.isTriggered()) {
    latch.await();
  }
  final org.apache.hadoop.fs.Path[] filesCreated=new org.apache.hadoop.fs.Path[NO_OF_FILES];
  for (int i=0; i < NO_OF_FILES; i++) {
    Tuple2<org.apache.hadoop.fs.Path,String> ignoredFile=createFileAndFillWithData(testBasePath,"file",i,"This is test line.");
    filesCreated[i]=ignoredFile.f0;
  }
  t.join();
  Assert.assertArrayEquals(filesToBeRead.toArray(),context.getSeenFiles().toArray());
  hdfs.delete(bootstrap.f0,false);
  for (  org.apache.hadoop.fs.Path path : filesCreated) {
    hdfs.delete(path,false);
  }
}
