@Test public void lossFunctionGradientCheckLossLayer(){
  ILossFunction[] lossFunctions=new ILossFunction[]{new LossBinaryXENT(),new LossBinaryXENT(),new LossCosineProximity(),new LossHinge(),new LossKLD(),new LossKLD(),new LossL1(),new LossL1(),new LossL2(),new LossL2(),new LossMAE(),new LossMAE(),new LossMAPE(),new LossMAPE(),new LossMCXENT(),new LossMSE(),new LossMSE(),new LossMSLE(),new LossMSLE(),new LossNegativeLogLikelihood(),new LossNegativeLogLikelihood(),new LossPoisson(),new LossSquaredHinge(),new LossFMeasure(),new LossFMeasure(2.0),new LossFMeasure(),new LossFMeasure(2.0),LossMixtureDensity.builder().gaussians(2).labelWidth(3).build(),LossMixtureDensity.builder().gaussians(2).labelWidth(3).build(),new LossMultiLabel()};
  Activation[] outputActivationFn=new Activation[]{Activation.SIGMOID,Activation.SIGMOID,Activation.TANH,Activation.TANH,Activation.SIGMOID,Activation.SOFTMAX,Activation.TANH,Activation.SOFTMAX,Activation.TANH,Activation.SOFTMAX,Activation.IDENTITY,Activation.SOFTMAX,Activation.IDENTITY,Activation.SOFTMAX,Activation.SOFTMAX,Activation.IDENTITY,Activation.SOFTMAX,Activation.SIGMOID,Activation.SOFTMAX,Activation.SIGMOID,Activation.SOFTMAX,Activation.SIGMOID,Activation.TANH,Activation.SIGMOID,Activation.SIGMOID,Activation.SOFTMAX,Activation.SOFTMAX,Activation.IDENTITY,Activation.TANH,Activation.TANH};
  int[] nOut=new int[]{1,3,5,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,1,1,2,2,10,10,10};
  int[] minibatchSizes=new int[]{1,3};
  List<String> passed=new ArrayList<>();
  List<String> failed=new ArrayList<>();
  for (int i=0; i < lossFunctions.length; i++) {
    for (int j=0; j < minibatchSizes.length; j++) {
      String testName=lossFunctions[i] + " - " + outputActivationFn[i]+ " - minibatchSize = "+ minibatchSizes[j];
      try {
        ObjectMapper m=NeuralNetConfiguration.mapper();
        String s=m.writeValueAsString(lossFunctions[i]);
        ILossFunction lf2=m.readValue(s,lossFunctions[i].getClass());
        lossFunctions[i]=lf2;
      }
 catch (      IOException ex) {
        ex.printStackTrace();
        assertEquals("Tests failed: serialization of " + lossFunctions[i],0,1);
      }
      Nd4j.getRandom().setSeed(12345);
      MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).seed(12345).updater(new NoOp()).weightInit(WeightInit.DISTRIBUTION).dist(new UniformDistribution(-2,2)).list().layer(0,new DenseLayer.Builder().nIn(4).nOut(nOut[i]).activation(Activation.TANH).build()).layer(1,new LossLayer.Builder().lossFunction(lossFunctions[i]).activation(outputActivationFn[i]).build()).validateOutputLayerConfig(false).build();
      MultiLayerNetwork net=new MultiLayerNetwork(conf);
      net.init();
      assertTrue(((LossLayer)net.getLayer(1).conf().getLayer()).getLossFn().getClass() == lossFunctions[i].getClass());
      INDArray[] inOut=getFeaturesAndLabels(lossFunctions[i],minibatchSizes[j],4,nOut[i],12345);
      INDArray input=inOut[0];
      INDArray labels=inOut[1];
      log.info(" ***** Starting test: {} *****",testName);
      boolean gradOK;
      try {
        gradOK=GradientCheckUtil.checkGradients(net,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,input,labels);
      }
 catch (      Exception e) {
        e.printStackTrace();
        failed.add(testName + "\t" + "EXCEPTION");
        continue;
      }
      if (gradOK) {
        passed.add(testName);
      }
 else {
        failed.add(testName);
      }
      System.out.println("\n\n");
      TestUtils.testModelSerialization(net);
    }
  }
  System.out.println("---- Passed ----");
  for (  String s : passed) {
    System.out.println(s);
  }
  System.out.println("---- Failed ----");
  for (  String s : failed) {
    System.out.println(s);
  }
  assertEquals("Tests failed",0,failed.size());
}
