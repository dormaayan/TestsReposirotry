@Test public void testL2NormalizeVertex4d(){
  Nd4j.getRandom().setSeed(12345);
  int h=4;
  int w=4;
  int dIn=2;
  ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().seed(12345).optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).activation(Activation.TANH).updater(new NoOp()).graphBuilder().addInputs("in1").addLayer("d1",new ConvolutionLayer.Builder().kernelSize(2,2).stride(1,1).nOut(2).build(),"in1").addVertex("norm",new L2NormalizeVertex(),"d1").addLayer("out1",new OutputLayer.Builder().lossFunction(LossFunctions.LossFunction.L2).nOut(2).activation(Activation.IDENTITY).build(),"norm").setOutputs("out1").setInputTypes(InputType.convolutional(h,w,dIn)).build();
  ComputationGraph graph=new ComputationGraph(conf);
  graph.init();
  int[] mbSizes=new int[]{1,3,10};
  for (  int minibatch : mbSizes) {
    INDArray in1=Nd4j.rand(new int[]{minibatch,dIn,h,w});
    INDArray labels1=Nd4j.rand(minibatch,2);
    String testName="testL2NormalizeVertex4d() - minibatch = " + minibatch;
    if (PRINT_RESULTS) {
      System.out.println(testName);
      for (int j=0; j < graph.getNumLayers(); j++)       System.out.println("Layer " + j + " # params: "+ graph.getLayer(j).numParams());
    }
    boolean gradOK=GradientCheckUtil.checkGradients(graph,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,new INDArray[]{in1},new INDArray[]{labels1});
    assertTrue(testName,gradOK);
    TestUtils.testModelSerialization(graph);
  }
}
