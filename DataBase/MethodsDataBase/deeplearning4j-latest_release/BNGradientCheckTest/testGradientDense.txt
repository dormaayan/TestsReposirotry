@Test public void testGradientDense(){
  Activation[] activFns={Activation.SIGMOID,Activation.TANH,Activation.IDENTITY};
  boolean[] characteristic={false,true};
  LossFunctions.LossFunction[] lossFunctions={LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD,LossFunctions.LossFunction.MSE};
  Activation[] outputActivations={Activation.SOFTMAX,Activation.TANH};
  double[] l2vals={0.0,0.1,0.1};
  double[] l1vals={0.0,0.0,0.2};
  Nd4j.getRandom().setSeed(12345);
  int minibatch=10;
  int nIn=5;
  int nOut=3;
  INDArray input=Nd4j.rand(new int[]{minibatch,nIn});
  INDArray labels=Nd4j.zeros(minibatch,nOut);
  Random r=new Random(12345);
  for (int i=0; i < minibatch; i++) {
    labels.putScalar(i,r.nextInt(nOut),1.0);
  }
  DataSet ds=new DataSet(input,labels);
  for (  Activation afn : activFns) {
    for (    boolean doLearningFirst : characteristic) {
      for (int i=0; i < lossFunctions.length; i++) {
        for (int j=0; j < l2vals.length; j++) {
          LossFunctions.LossFunction lf=lossFunctions[i];
          Activation outputActivation=outputActivations[i];
          MultiLayerConfiguration.Builder builder=new NeuralNetConfiguration.Builder().l2(l2vals[j]).optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT).updater(new NoOp()).weightInit(WeightInit.DISTRIBUTION).dist(new UniformDistribution(-2,2)).seed(12345L).list().layer(0,new DenseLayer.Builder().nIn(nIn).nOut(4).activation(afn).build()).layer(1,new BatchNormalization.Builder().build()).layer(2,new DenseLayer.Builder().nIn(4).nOut(4).build()).layer(3,new BatchNormalization()).layer(4,new OutputLayer.Builder(lf).activation(outputActivation).nOut(nOut).build());
          MultiLayerConfiguration conf=builder.build();
          MultiLayerNetwork mln=new MultiLayerNetwork(conf);
          mln.init();
          String name=new Object(){
          }
.getClass().getEnclosingMethod().getName();
          if (doLearningFirst) {
            mln.setInput(ds.getFeatures());
            mln.setLabels(ds.getLabels());
            mln.computeGradientAndScore();
            double scoreBefore=mln.score();
            for (int k=0; k < 10; k++)             mln.fit(ds);
            mln.computeGradientAndScore();
            double scoreAfter=mln.score();
            String msg=name + " - score did not (sufficiently) decrease during learning - activationFn=" + afn+ ", lossFn="+ lf+ ", outputActivation="+ outputActivation+ ", doLearningFirst= "+ doLearningFirst+ " (before="+ scoreBefore+ ", scoreAfter="+ scoreAfter+ ")";
            assertTrue(msg,scoreAfter < 0.8 * scoreBefore);
          }
          if (PRINT_RESULTS) {
            System.out.println(name + " - activationFn=" + afn+ ", lossFn="+ lf+ ", outputActivation="+ outputActivation+ ", doLearningFirst="+ doLearningFirst+ ", l1="+ l1vals[j]+ ", l2="+ l2vals[j]);
            for (int k=0; k < mln.getnLayers(); k++)             System.out.println("Layer " + k + " # params: "+ mln.getLayer(k).numParams());
          }
          Set<String> excludeParams=new HashSet<>(Arrays.asList("1_mean","1_var","3_mean","3_var"));
          boolean gradOK=GradientCheckUtil.checkGradients(mln,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,input,labels,excludeParams);
          assertTrue(gradOK);
          TestUtils.testModelSerialization(mln);
        }
      }
    }
  }
}
