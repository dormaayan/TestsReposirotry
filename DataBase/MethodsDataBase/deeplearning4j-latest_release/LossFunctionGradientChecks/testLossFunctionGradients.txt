@Test public void testLossFunctionGradients(){
  INDArray[] labels=new INDArray[]{Nd4j.create(new double[]{0,1,0}),Nd4j.create(new double[]{0,1,1}),Nd4j.create(new double[]{1,2,1}),Nd4j.create(new double[][]{{101,21,110},{10.1,1,0.5},{200,30,0.001}}),Nd4j.create(new double[]{1,2,1}),Nd4j.create(new double[][]{{101,21,110},{10.1,1,0.5},{200,30,0.001}}),Nd4j.create(new double[]{1,2,1}),Nd4j.create(new double[][]{{101,21,110},{10.1,1,0.5},{200,30,0.001}}),Nd4j.create(new double[]{1,2,1}),Nd4j.create(new double[][]{{101,21,110},{10.1,1,0.5},{200,30,0.001}}),Nd4j.create(new double[][]{{-1,1,-1},{1,1,-1},{-1,1,1}}),Nd4j.create(new double[][]{{-1,1,-1},{1,1,-1},{-1,1,1}}),Nd4j.create(new double[][]{{1,0,0},{0,1,0},{1,0,1}})};
  INDArray[] preOut=new INDArray[]{Nd4j.rand(1,3),Nd4j.rand(1,3),Nd4j.randn(1,3),Nd4j.randn(3,3).add(10),Nd4j.rand(1,3),Nd4j.randn(3,3).add(10),Nd4j.randn(1,3),Nd4j.randn(3,3).add(10),Nd4j.rand(1,3),Nd4j.randn(3,3).add(10),Nd4j.rand(3,3).addi(-0.5),Nd4j.rand(3,3).addi(-0.5),Nd4j.rand(3,3).addi(-0.5)};
  ILossFunction[] lossFn=new ILossFunction[]{new LossBinaryXENT(),new LossBinaryXENT(),new LossMAE(),new LossMAE(),new LossMSE(),new LossMSE(),new LossL1(),new LossL1(),new LossL2(),new LossL2(),new LossSquaredHinge(),new LossHinge(),new LossMultiLabel()};
  String[] activationFns=new String[]{"identity","tanh","identity","identity","identity","identity","sigmoid","relu","sigmoid","relu","identity","identity","identity"};
  for (int i=0; i < labels.length; i++) {
    int totalNFailures=0;
    ILossFunction lf=lossFn[i];
    INDArray l=labels[i];
    INDArray p=preOut[i];
    String afn=activationFns[i];
    System.out.printf("Starting test: %s, %s, input shape = %s\n",lf,afn,Arrays.toString(p.shape()));
    INDArray grad=lf.computeGradient(l,p,activationInstance(afn),null);
    NdIndexIterator iter=new NdIndexIterator(l.shape());
    while (iter.hasNext()) {
      val next=iter.next();
      double before=p.getDouble(next);
      p.putScalar(next,before + epsilon);
      double scorePlus=lf.computeScore(l,p,activationInstance(afn),null,true);
      p.putScalar(next,before - epsilon);
      double scoreMinus=lf.computeScore(l,p,activationInstance(afn),null,true);
      p.putScalar(next,before);
      double scoreDelta=scorePlus - scoreMinus;
      double numericalGradient=scoreDelta / (2 * epsilon);
      double analyticGradient=grad.getDouble(next) / l.size(0);
      double relError=Math.abs(analyticGradient - numericalGradient) * 100 / (Math.abs(numericalGradient));
      if (analyticGradient == 0.0 && numericalGradient == 0.0)       relError=0.0;
      if (relError > maxRelError || Double.isNaN(relError)) {
        System.out.println("Param " + i + " FAILED: grad= "+ analyticGradient+ ", numericalGrad= "+ numericalGradient+ ", relErrorPerc= "+ relError+ ", scorePlus="+ scorePlus+ ", scoreMinus= "+ scoreMinus);
        totalNFailures++;
      }
 else {
        System.out.println("Param " + i + " passed: grad= "+ analyticGradient+ ", numericalGrad= "+ numericalGradient+ ", relError= "+ relError+ ", scorePlus="+ scorePlus+ ", scoreMinus= "+ scoreMinus);
      }
    }
    if (totalNFailures > 0)     System.out.println("Gradient check failed for loss function " + lf + "; total num failures = "+ totalNFailures);
    System.out.println("DONE");
  }
}
