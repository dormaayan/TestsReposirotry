@Test public void testRecordReaderBytesFunction() throws Exception {
  JavaSparkContext sc=getContext();
  File f=testDir.newFolder();
  new ClassPathResource("datavec-spark/imagetest/").copyDirectory(f);
  List<String> labelsList=Arrays.asList("0","1");
  String path=f.getAbsolutePath() + "/*";
  JavaPairRDD<String,PortableDataStream> origData=sc.binaryFiles(path);
  JavaPairRDD<Text,BytesWritable> filesAsBytes=origData.mapToPair(new FilesAsBytesFunction());
  Path p=Files.createTempDirectory("dl4j_rrbytesTest");
  p.toFile().deleteOnExit();
  String outPath=p.toString() + "/out";
  filesAsBytes.saveAsNewAPIHadoopFile(outPath,Text.class,BytesWritable.class,SequenceFileOutputFormat.class);
  JavaPairRDD<Text,BytesWritable> fromSeqFile=sc.sequenceFile(outPath,Text.class,BytesWritable.class);
  ImageRecordReader irr=new ImageRecordReader(28,28,1,new ParentPathLabelGenerator());
  irr.setLabels(labelsList);
  JavaRDD<List<Writable>> dataVecData=fromSeqFile.map(new RecordReaderBytesFunction(irr));
  InputSplit is=new FileSplit(f,new String[]{"bmp"},true);
  irr=new ImageRecordReader(28,28,1,new ParentPathLabelGenerator());
  irr.initialize(is);
  List<List<Writable>> list=new ArrayList<>(4);
  while (irr.hasNext()) {
    list.add(irr.next());
  }
  List<List<Writable>> fromSequenceFile=dataVecData.collect();
  assertEquals(4,list.size());
  assertEquals(4,fromSequenceFile.size());
  boolean[] found=new boolean[4];
  for (int i=0; i < 4; i++) {
    int foundIndex=-1;
    List<Writable> collection=fromSequenceFile.get(i);
    for (int j=0; j < 4; j++) {
      if (collection.equals(list.get(j))) {
        if (foundIndex != -1)         fail();
        foundIndex=j;
        if (found[foundIndex])         fail();
        found[foundIndex]=true;
      }
    }
  }
  int count=0;
  for (  boolean b : found)   if (b)   count++;
  assertEquals(4,count);
}
