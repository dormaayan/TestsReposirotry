@Test public void testAdamUpdater(){
  INDArray m, v;
  double lr=0.01;
  int iteration=0;
  double beta1=0.8;
  double beta2=0.888;
  double epsilon=Adam.DEFAULT_ADAM_EPSILON;
  NeuralNetConfiguration conf=new NeuralNetConfiguration.Builder().updater(new Adam(lr,beta1,beta2,Adam.DEFAULT_ADAM_EPSILON)).layer(new DenseLayer.Builder().nIn(nIn).nOut(nOut).build()).build();
  long numParams=conf.getLayer().initializer().numParams(conf);
  INDArray params=Nd4j.create(1,numParams);
  BaseLayer layer=(BaseLayer)conf.getLayer().instantiate(conf,null,0,params,true);
  layer.setBackpropGradientsViewArray(gradients);
  Updater updater=UpdaterCreator.getUpdater(layer);
  int updaterStateSize=(int)layer.layerConf().getIUpdater().stateSize(numParams);
  INDArray updaterState=Nd4j.create(1,updaterStateSize);
  updater.setStateViewArray(layer,updaterState,true);
  updater.update(layer,gradient,iteration,0,1,LayerWorkspaceMgr.noWorkspaces());
  double beta1t=FastMath.pow(beta1,iteration + 1);
  double beta2t=FastMath.pow(beta2,iteration + 1);
  double alphat=lr * FastMath.sqrt(1 - beta2t) / (1 - beta1t);
  if (Double.isNaN(alphat) || alphat == 0.0)   alphat=epsilon;
  Gradient gradientCopyPreUpdate=new DefaultGradient();
  INDArray g=gradients.dup();
  INDArray wg=g.get(point(0),interval(0,nIn * nOut));
  INDArray bg=g.get(point(0),interval(nIn * nOut,nIn * nOut + nOut));
  gradientCopyPreUpdate.setGradientFor(DefaultParamInitializer.WEIGHT_KEY,wg);
  gradientCopyPreUpdate.setGradientFor(DefaultParamInitializer.BIAS_KEY,bg);
  int count=0;
  for (  Map.Entry<String,INDArray> entry : gradientCopyPreUpdate.gradientForVariable().entrySet()) {
    val=entry.getValue();
    m=Nd4j.zeros(val.shape());
    v=Nd4j.zeros(val.shape());
    m.muli(beta1).addi(val.mul(1.0 - beta1));
    v.muli(beta2).addi(val.mul(val).mul(1.0 - beta2));
    gradExpected=m.mul(alphat).divi(Transforms.sqrt(v).addi(epsilon));
    if (!gradExpected.equals(gradient.getGradientFor(entry.getKey()))) {
      System.out.println(Arrays.toString(gradExpected.dup().data().asFloat()));
      System.out.println(Arrays.toString(gradient.getGradientFor(entry.getKey()).dup().data().asFloat()));
    }
    assertEquals(gradExpected,gradient.getGradientFor(entry.getKey()));
    count++;
  }
  assertEquals(beta1,((Adam)layer.layerConf().getIUpdater()).getBeta1(),1e-4);
  assertEquals(beta2,((Adam)layer.layerConf().getIUpdater()).getBeta2(),1e-4);
  assertEquals(2,count);
}
