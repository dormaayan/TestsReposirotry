@Test public void testEvaluationSimple() throws Exception {
  for (  int evalWorkers : new int[]{1,4,8}) {
    int numEpochs=1;
    int batchSizePerWorker=8;
    DataSetIterator iterTrain=new MnistDataSetIterator(batchSizePerWorker,true,12345);
    DataSetIterator iterTest=new MnistDataSetIterator(batchSizePerWorker,false,12345);
    List<DataSet> trainDataList=new ArrayList<>();
    List<DataSet> testDataList=new ArrayList<>();
    int count=0;
    while (iterTrain.hasNext() && count++ < 30) {
      trainDataList.add(iterTrain.next());
    }
    while (iterTest.hasNext()) {
      testDataList.add(iterTest.next());
    }
    JavaRDD<DataSet> trainData=sc.parallelize(trainDataList);
    JavaRDD<DataSet> testData=sc.parallelize(testDataList);
    MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().seed(12345).optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).activation(Activation.LEAKYRELU).weightInit(WeightInit.XAVIER).updater(new Nesterovs(0.02,0.9)).l2(1e-4).list().layer(0,new DenseLayer.Builder().nIn(28 * 28).nOut(500).build()).layer(1,new DenseLayer.Builder().nIn(500).nOut(100).build()).layer(2,new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD).activation(Activation.SOFTMAX).nIn(100).nOut(10).build()).build();
    TrainingMaster tm=new ParameterAveragingTrainingMaster.Builder(batchSizePerWorker).averagingFrequency(2).build();
    SparkDl4jMultiLayer sparkNet=new SparkDl4jMultiLayer(sc,conf,tm);
    sparkNet.setDefaultEvaluationWorkers(evalWorkers);
    for (int i=0; i < numEpochs; i++) {
      sparkNet.fit(trainData);
    }
    Evaluation evaluation=sparkNet.evaluate(testData);
    log.info("***** Evaluation *****");
    log.info(evaluation.stats());
    tm.deleteTempFiles(sc);
    assertEquals(10000,evaluation.getNumRowCounter());
    assertTrue(!Double.isNaN(evaluation.accuracy()));
    assertTrue(evaluation.accuracy() >= 0.10);
    assertTrue(evaluation.precision() >= 0.10);
    assertTrue(evaluation.recall() >= 0.10);
    assertTrue(evaluation.f1() >= 0.10);
  }
}
