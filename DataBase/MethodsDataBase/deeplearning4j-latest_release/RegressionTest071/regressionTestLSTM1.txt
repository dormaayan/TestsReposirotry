@Test public void regressionTestLSTM1() throws Exception {
  File f=new ClassPathResource("regression_testing/071/071_ModelSerializer_Regression_LSTM_1.zip").getTempFileFromArchive();
  MultiLayerNetwork net=ModelSerializer.restoreMultiLayerNetwork(f,true);
  MultiLayerConfiguration conf=net.getLayerWiseConfigurations();
  assertEquals(3,conf.getConfs().size());
  assertTrue(conf.isBackprop());
  assertFalse(conf.isPretrain());
  GravesLSTM l0=(GravesLSTM)conf.getConf(0).getLayer();
  assertEquals("tanh",l0.getActivationFn().toString());
  assertEquals(3,l0.getNIn());
  assertEquals(4,l0.getNOut());
  assertEquals(GradientNormalization.ClipElementWiseAbsoluteValue,l0.getGradientNormalization());
  assertEquals(1.5,l0.getGradientNormalizationThreshold(),1e-5);
  GravesBidirectionalLSTM l1=(GravesBidirectionalLSTM)conf.getConf(1).getLayer();
  assertEquals("softsign",l1.getActivationFn().toString());
  assertEquals(4,l1.getNIn());
  assertEquals(4,l1.getNOut());
  assertEquals(GradientNormalization.ClipElementWiseAbsoluteValue,l1.getGradientNormalization());
  assertEquals(1.5,l1.getGradientNormalizationThreshold(),1e-5);
  RnnOutputLayer l2=(RnnOutputLayer)conf.getConf(2).getLayer();
  assertEquals(4,l2.getNIn());
  assertEquals(5,l2.getNOut());
  assertEquals("softmax",l2.getActivationFn().toString());
  assertTrue(l2.getLossFn() instanceof LossMCXENT);
}
