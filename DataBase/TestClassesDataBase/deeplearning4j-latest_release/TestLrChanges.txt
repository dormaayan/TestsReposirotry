public class TestLrChanges extends BaseDL4JTest {
  @Test public void testChangeLrMLN(){
    MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().activation(Activation.TANH).seed(12345).list().layer(new DenseLayer.Builder().nIn(10).nOut(10).updater(new Adam(0.1)).build()).layer(new DenseLayer.Builder().nIn(10).nOut(10).updater(new RmsProp(0.01)).build()).layer(new OutputLayer.Builder().nIn(10).nOut(10).updater(new NoOp()).lossFunction(LossFunctions.LossFunction.MSE).build()).build();
    MultiLayerNetwork net=new MultiLayerNetwork(conf);
    net.init();
    for (int i=0; i < 10; i++) {
      net.fit(Nd4j.rand(10,10),Nd4j.rand(10,10));
    }
    MultiLayerConfiguration conf2=new NeuralNetConfiguration.Builder().activation(Activation.TANH).seed(12345).list().layer(new DenseLayer.Builder().nIn(10).nOut(10).updater(new Adam(0.5)).build()).layer(new DenseLayer.Builder().nIn(10).nOut(10).updater(new RmsProp(0.01)).build()).layer(new OutputLayer.Builder().nIn(10).nOut(10).updater(new NoOp()).lossFunction(LossFunctions.LossFunction.MSE).build()).build();
    MultiLayerNetwork net2=new MultiLayerNetwork(conf2);
    net2.init();
    net2.getUpdater().getStateViewArray().assign(net.getUpdater().getStateViewArray());
    conf2.setIterationCount(conf.getIterationCount());
    net2.setParams(net.params().dup());
    assertEquals(0.1,net.getLearningRate(0).doubleValue(),0.0);
    net.setLearningRate(0,0.5);
    assertEquals(0.5,net.getLearningRate(0).doubleValue(),0.0);
    assertEquals(conf,conf2);
    assertEquals(conf.toJson(),conf2.toJson());
    assertEquals(net.getUpdater().getStateViewArray(),net2.getUpdater().getStateViewArray());
    for (int i=0; i < 3; i++) {
      INDArray in=Nd4j.rand(10,10);
      INDArray l=Nd4j.rand(10,10);
      net.fit(in,l);
      net2.fit(in,l);
    }
    assertEquals(net.params(),net2.params());
    assertEquals(net.getUpdater().getStateViewArray(),net2.getUpdater().getStateViewArray());
    INDArray in1=Nd4j.rand(10,10);
    INDArray l1=Nd4j.rand(10,10);
    net.setInput(in1);
    net.setLabels(l1);
    net.computeGradientAndScore();
    net2.setInput(in1);
    net2.setLabels(l1);
    net2.computeGradientAndScore();
    assertEquals(net.score(),net2.score(),1e-8);
    MultiLayerConfiguration conf3=new NeuralNetConfiguration.Builder().activation(Activation.TANH).seed(12345).list().layer(new DenseLayer.Builder().nIn(10).nOut(10).updater(new Adam(0.3)).build()).layer(new DenseLayer.Builder().nIn(10).nOut(10).updater(new RmsProp(0.3)).build()).layer(new OutputLayer.Builder().nIn(10).nOut(10).updater(new NoOp()).lossFunction(LossFunctions.LossFunction.MSE).build()).build();
    MultiLayerNetwork net3=new MultiLayerNetwork(conf3);
    net3.init();
    net3.getUpdater().getStateViewArray().assign(net.getUpdater().getStateViewArray());
    conf3.setIterationCount(conf.getIterationCount());
    net3.setParams(net.params().dup());
    net.setLearningRate(0.3);
    for (int i=0; i < 3; i++) {
      INDArray in=Nd4j.rand(10,10);
      INDArray l=Nd4j.rand(10,10);
      net.fit(in,l);
      net3.fit(in,l);
    }
    assertEquals(net.params(),net3.params());
    assertEquals(net.getUpdater().getStateViewArray(),net3.getUpdater().getStateViewArray());
  }
  @Test public void testChangeLSGD(){
    MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().activation(Activation.TANH).seed(12345).updater(new Sgd(0.1)).list().layer(new DenseLayer.Builder().nIn(10).nOut(10).build()).layer(new DenseLayer.Builder().nIn(10).nOut(10).build()).layer(new OutputLayer.Builder().nIn(10).nOut(10).lossFunction(LossFunctions.LossFunction.MSE).build()).build();
    MultiLayerNetwork net=new MultiLayerNetwork(conf);
    net.init();
    net.setLearningRate(1.0);
    net.setLearningRate(1,0.5);
    assertEquals(1.0,net.getLearningRate(0),0.0);
    assertEquals(0.5,net.getLearningRate(1),0.0);
    ComputationGraph cg=net.toComputationGraph();
    cg.setLearningRate(2.0);
    cg.setLearningRate("1",2.5);
    assertEquals(2.0,cg.getLearningRate("0"),0.0);
    assertEquals(2.5,cg.getLearningRate("1"),0.0);
  }
  @Test public void testChangeLrMLNSchedule(){
    MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().activation(Activation.TANH).seed(12345).updater(new Adam(0.1)).list().layer(new DenseLayer.Builder().nIn(10).nOut(10).build()).layer(new DenseLayer.Builder().nIn(10).nOut(10).build()).layer(new OutputLayer.Builder().nIn(10).nOut(10).activation(Activation.SOFTMAX).build()).build();
    MultiLayerNetwork net=new MultiLayerNetwork(conf);
    net.init();
    for (int i=0; i < 10; i++) {
      net.fit(Nd4j.rand(10,10),Nd4j.rand(10,10));
    }
    MultiLayerConfiguration conf2=new NeuralNetConfiguration.Builder().activation(Activation.TANH).seed(12345).updater(new Adam(new ExponentialSchedule(ScheduleType.ITERATION,0.5,0.8))).list().layer(new DenseLayer.Builder().nIn(10).nOut(10).build()).layer(new DenseLayer.Builder().nIn(10).nOut(10).build()).layer(new OutputLayer.Builder().nIn(10).nOut(10).activation(Activation.SOFTMAX).build()).build();
    MultiLayerNetwork net2=new MultiLayerNetwork(conf2);
    net2.init();
    net2.getUpdater().getStateViewArray().assign(net.getUpdater().getStateViewArray());
    conf2.setIterationCount(conf.getIterationCount());
    net2.setParams(net.params().dup());
    net.setLearningRate(new ExponentialSchedule(ScheduleType.ITERATION,0.5,0.8));
    assertEquals(conf,conf2);
    assertEquals(conf.toJson(),conf2.toJson());
    assertEquals(net.getUpdater().getStateViewArray(),net2.getUpdater().getStateViewArray());
    for (int i=0; i < 3; i++) {
      INDArray in=Nd4j.rand(10,10);
      INDArray l=Nd4j.rand(10,10);
      net.fit(in,l);
      net2.fit(in,l);
    }
    assertEquals(net.params(),net2.params());
    assertEquals(net.getUpdater().getStateViewArray(),net2.getUpdater().getStateViewArray());
  }
  @Test public void testChangeLrCompGraph(){
    ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().activation(Activation.TANH).seed(12345).graphBuilder().addInputs("in").addLayer("0",new DenseLayer.Builder().nIn(10).nOut(10).updater(new Adam(0.1)).build(),"in").addLayer("1",new DenseLayer.Builder().nIn(10).nOut(10).updater(new RmsProp(0.01)).build(),"0").addLayer("2",new OutputLayer.Builder().nIn(10).nOut(10).updater(new NoOp()).lossFunction(LossFunctions.LossFunction.MSE).build(),"1").setOutputs("2").build();
    ComputationGraph net=new ComputationGraph(conf);
    net.init();
    for (int i=0; i < 10; i++) {
      net.fit(new DataSet(Nd4j.rand(10,10),Nd4j.rand(10,10)));
    }
    ComputationGraphConfiguration conf2=new NeuralNetConfiguration.Builder().activation(Activation.TANH).seed(12345).graphBuilder().addInputs("in").addLayer("0",new DenseLayer.Builder().nIn(10).nOut(10).updater(new Adam(0.5)).build(),"in").addLayer("1",new DenseLayer.Builder().nIn(10).nOut(10).updater(new RmsProp(0.01)).build(),"0").addLayer("2",new OutputLayer.Builder().nIn(10).nOut(10).updater(new NoOp()).lossFunction(LossFunctions.LossFunction.MSE).build(),"1").setOutputs("2").build();
    ComputationGraph net2=new ComputationGraph(conf2);
    net2.init();
    net2.getUpdater().getStateViewArray().assign(net.getUpdater().getStateViewArray());
    conf2.setIterationCount(conf.getIterationCount());
    net2.setParams(net.params().dup());
    assertEquals(0.1,net.getLearningRate("0").doubleValue(),0.0);
    net.setLearningRate("0",0.5);
    assertEquals(0.5,net.getLearningRate("0").doubleValue(),0.0);
    assertEquals(conf,conf2);
    assertEquals(conf.toJson(),conf2.toJson());
    assertEquals(net.getUpdater().getStateViewArray(),net2.getUpdater().getStateViewArray());
    for (int i=0; i < 3; i++) {
      INDArray in=Nd4j.rand(10,10);
      INDArray l=Nd4j.rand(10,10);
      net.fit(new DataSet(in,l));
      net2.fit(new DataSet(in,l));
    }
    assertEquals(net.params(),net2.params());
    assertEquals(net.getUpdater().getStateViewArray(),net2.getUpdater().getStateViewArray());
    INDArray in1=Nd4j.rand(10,10);
    INDArray l1=Nd4j.rand(10,10);
    net.setInputs(in1);
    net.setLabels(l1);
    net.computeGradientAndScore();
    net2.setInputs(in1);
    net2.setLabels(l1);
    net2.computeGradientAndScore();
    assertEquals(net.score(),net2.score(),1e-8);
    MultiLayerConfiguration conf3=new NeuralNetConfiguration.Builder().activation(Activation.TANH).seed(12345).list().layer(new DenseLayer.Builder().nIn(10).nOut(10).updater(new Adam(0.3)).build()).layer(new DenseLayer.Builder().nIn(10).nOut(10).updater(new RmsProp(0.3)).build()).layer(new OutputLayer.Builder().nIn(10).nOut(10).updater(new NoOp()).lossFunction(LossFunctions.LossFunction.MSE).build()).build();
    MultiLayerNetwork net3=new MultiLayerNetwork(conf3);
    net3.init();
    net3.getUpdater().getStateViewArray().assign(net.getUpdater().getStateViewArray());
    conf3.setIterationCount(conf.getIterationCount());
    net3.setParams(net.params().dup());
    net.setLearningRate(0.3);
    for (int i=0; i < 3; i++) {
      INDArray in=Nd4j.rand(10,10);
      INDArray l=Nd4j.rand(10,10);
      net.fit(new DataSet(in,l));
      net3.fit(new DataSet(in,l));
    }
    assertEquals(net.params(),net3.params());
    assertEquals(net.getUpdater().getStateViewArray(),net3.getUpdater().getStateViewArray());
  }
  @Test public void testChangeLrCompGraphSchedule(){
    ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().activation(Activation.TANH).seed(12345).updater(new Adam(0.1)).graphBuilder().addInputs("in").addLayer("0",new DenseLayer.Builder().nIn(10).nOut(10).build(),"in").addLayer("1",new DenseLayer.Builder().nIn(10).nOut(10).build(),"0").addLayer("2",new OutputLayer.Builder().nIn(10).nOut(10).activation(Activation.SOFTMAX).build(),"1").setOutputs("2").build();
    ComputationGraph net=new ComputationGraph(conf);
    net.init();
    for (int i=0; i < 10; i++) {
      net.fit(new DataSet(Nd4j.rand(10,10),Nd4j.rand(10,10)));
    }
    ComputationGraphConfiguration conf2=new NeuralNetConfiguration.Builder().activation(Activation.TANH).seed(12345).updater(new Adam(new ExponentialSchedule(ScheduleType.ITERATION,0.5,0.8))).graphBuilder().addInputs("in").addLayer("0",new DenseLayer.Builder().nIn(10).nOut(10).build(),"in").addLayer("1",new DenseLayer.Builder().nIn(10).nOut(10).build(),"0").layer("2",new OutputLayer.Builder().nIn(10).nOut(10).activation(Activation.SOFTMAX).build(),"1").setOutputs("2").build();
    ComputationGraph net2=new ComputationGraph(conf2);
    net2.init();
    net2.getUpdater().getStateViewArray().assign(net.getUpdater().getStateViewArray());
    conf2.setIterationCount(conf.getIterationCount());
    net2.setParams(net.params().dup());
    net.setLearningRate(new ExponentialSchedule(ScheduleType.ITERATION,0.5,0.8));
    assertEquals(conf,conf2);
    assertEquals(conf.toJson(),conf2.toJson());
    assertEquals(net.getUpdater().getStateViewArray(),net2.getUpdater().getStateViewArray());
    for (int i=0; i < 3; i++) {
      INDArray in=Nd4j.rand(10,10);
      INDArray l=Nd4j.rand(10,10);
      net.fit(new DataSet(in,l));
      net2.fit(new DataSet(in,l));
    }
    assertEquals(net.params(),net2.params());
    assertEquals(net.getUpdater().getStateViewArray(),net2.getUpdater().getStateViewArray());
  }
}
