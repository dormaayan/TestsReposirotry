/** 
 * This test verifies that incremental block reports from a single DataNode are correctly handled by NN. Tests the following variations: #1 - Incremental BRs from all storages combined in a single call. #2 - Incremental BRs from separate storages sent in separate calls. #3 - Incremental BR from an unknown storage should be rejected. We also verify that the DataNode is not splitting the reports (it may do so in the future).
 */
public class TestIncrementalBrVariations {
  public static final Logger LOG=LoggerFactory.getLogger(TestIncrementalBrVariations.class);
  private static final short NUM_DATANODES=1;
  static final int BLOCK_SIZE=1024;
  static final int NUM_BLOCKS=10;
  private static final long seed=0xFACEFEEDL;
  private static final String NN_METRICS="NameNodeActivity";
  private MiniDFSCluster cluster;
  private DistributedFileSystem fs;
  private DFSClient client;
  private static Configuration conf;
  private String poolId;
  private DataNode dn0;
  private DatanodeRegistration dn0Reg;
static {
    GenericTestUtils.setLogLevel(NameNode.stateChangeLog,Level.TRACE);
    GenericTestUtils.setLogLevel(BlockManager.blockLog,Level.TRACE);
    GenericTestUtils.setLogLevel(NameNode.blockStateChangeLog,Level.TRACE);
    GenericTestUtils.setLogLevel(LoggerFactory.getLogger(FSNamesystem.class),Level.TRACE);
    GenericTestUtils.setLogLevel(DataNode.LOG,Level.TRACE);
    GenericTestUtils.setLogLevel(TestIncrementalBrVariations.LOG,Level.TRACE);
  }
  @Before public void startUpCluster() throws IOException {
    conf=new Configuration();
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DATANODES).build();
    fs=cluster.getFileSystem();
    client=new DFSClient(new InetSocketAddress("localhost",cluster.getNameNodePort()),cluster.getConfiguration(0));
    dn0=cluster.getDataNodes().get(0);
    poolId=cluster.getNamesystem().getBlockPoolId();
    dn0Reg=dn0.getDNRegistrationForBP(poolId);
  }
  @After public void shutDownCluster() throws IOException {
    if (cluster != null) {
      client.close();
      fs.close();
      cluster.shutdownDataNodes();
      cluster.shutdown();
      cluster=null;
    }
  }
  /** 
 * Incremental BRs from all storages combined in a single message.
 */
  @Test public void testCombinedIncrementalBlockReport() throws IOException {
    verifyIncrementalBlockReports(false);
  }
  /** 
 * One incremental BR per storage.
 */
  @Test public void testSplitIncrementalBlockReport() throws IOException {
    verifyIncrementalBlockReports(true);
  }
  private LocatedBlocks createFileGetBlocks(  String filenamePrefix) throws IOException {
    Path filePath=new Path("/" + filenamePrefix + ".dat");
    DFSTestUtil.createFile(fs,filePath,BLOCK_SIZE,BLOCK_SIZE * NUM_BLOCKS,BLOCK_SIZE,NUM_DATANODES,seed);
    LocatedBlocks blocks=client.getLocatedBlocks(filePath.toString(),0,BLOCK_SIZE * NUM_BLOCKS);
    assertThat(cluster.getNamesystem().getUnderReplicatedBlocks(),is(0L));
    return blocks;
  }
  public void verifyIncrementalBlockReports(  boolean splitReports) throws IOException {
    LocatedBlocks blocks=createFileGetBlocks(GenericTestUtils.getMethodName());
    try (FsDatasetSpi.FsVolumeReferences volumes=dn0.getFSDataset().getFsVolumeReferences()){
      StorageReceivedDeletedBlocks reports[]=new StorageReceivedDeletedBlocks[volumes.size()];
      for (int i=0; i < reports.length; ++i) {
        FsVolumeSpi volume=volumes.get(i);
        boolean foundBlockOnStorage=false;
        ReceivedDeletedBlockInfo rdbi[]=new ReceivedDeletedBlockInfo[1];
        for (        LocatedBlock block : blocks.getLocatedBlocks()) {
          if (block.getStorageIDs()[0].equals(volume.getStorageID())) {
            rdbi[0]=new ReceivedDeletedBlockInfo(block.getBlock().getLocalBlock(),ReceivedDeletedBlockInfo.BlockStatus.DELETED_BLOCK,null);
            foundBlockOnStorage=true;
            break;
          }
        }
        assertTrue(foundBlockOnStorage);
        reports[i]=new StorageReceivedDeletedBlocks(new DatanodeStorage(volume.getStorageID()),rdbi);
        if (splitReports) {
          StorageReceivedDeletedBlocks singletonReport[]={reports[i]};
          cluster.getNameNodeRpc().blockReceivedAndDeleted(dn0Reg,poolId,singletonReport);
        }
      }
      if (!splitReports) {
        cluster.getNameNodeRpc().blockReceivedAndDeleted(dn0Reg,poolId,reports);
      }
      cluster.getNamesystem().getBlockManager().flushBlockOps();
      assertThat(cluster.getNamesystem().getMissingBlocksCount(),is((long)reports.length));
    }
   }
  /** 
 * Verify that the DataNode sends a single incremental block report for all storages.
 * @throws IOException
 * @throws InterruptedException
 */
  @Test(timeout=60000) public void testDataNodeDoesNotSplitReports() throws IOException, InterruptedException {
    LocatedBlocks blocks=createFileGetBlocks(GenericTestUtils.getMethodName());
    assertThat(cluster.getDataNodes().size(),is(1));
    for (    LocatedBlock block : blocks.getLocatedBlocks()) {
      dn0.notifyNamenodeDeletedBlock(block.getBlock(),block.getStorageIDs()[0]);
    }
    LOG.info("Triggering report after deleting blocks");
    long ops=getLongCounter("BlockReceivedAndDeletedOps",getMetrics(NN_METRICS));
    DataNodeTestUtils.triggerBlockReport(dn0);
    Thread.sleep(5000);
    assertCounter("BlockReceivedAndDeletedOps",ops + 1,getMetrics(NN_METRICS));
  }
  private static Block getDummyBlock(){
    return new Block(10000000L,100L,1048576L);
  }
  /** 
 * Verify that the NameNode can learn about new storages from incremental block reports. This tests the fix for the error condition seen in HDFS-6904.
 * @throws IOException
 * @throws InterruptedException
 */
  @Test(timeout=60000) public void testNnLearnsNewStorages() throws IOException, InterruptedException {
    final String newStorageUuid=UUID.randomUUID().toString();
    final DatanodeStorage newStorage=new DatanodeStorage(newStorageUuid);
    StorageReceivedDeletedBlocks[] reports=DFSTestUtil.makeReportForReceivedBlock(getDummyBlock(),BlockStatus.RECEIVED_BLOCK,newStorage);
    cluster.getNameNodeRpc().blockReceivedAndDeleted(dn0Reg,poolId,reports);
    cluster.getNamesystem().getBlockManager().flushBlockOps();
    DatanodeStorageInfo storageInfo=cluster.getNameNode().getNamesystem().getBlockManager().getDatanodeManager().getDatanode(dn0.getDatanodeId()).getStorageInfo(newStorageUuid);
    assertNotNull(storageInfo);
  }
}
