public class TestParquetRowGroupFilter extends AbstractTestParquetDirect {
  JobConf conf;
  String columnNames;
  String columnTypes;
  @Before public void initConf() throws Exception {
    conf=new JobConf();
  }
  @Test public void testRowGroupFilterTakeEffect() throws Exception {
    columnNames="intCol";
    columnTypes="int";
    StructObjectInspector inspector=getObjectInspector(columnNames,columnTypes);
    MessageType fileSchema=MessageTypeParser.parseMessageType("message hive_schema {\n" + "  optional int32 intCol;\n" + "}\n");
    conf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR,"intCol");
    conf.set("columns","intCol");
    conf.set("columns.types","int");
    Path testPath=writeDirect("RowGroupFilterTakeEffect",fileSchema,new DirectWriter(){
      @Override public void write(      RecordConsumer consumer){
        for (int i=0; i < 100; i++) {
          consumer.startMessage();
          consumer.startField("int",0);
          consumer.addInteger(i);
          consumer.endField("int",0);
          consumer.endMessage();
        }
      }
    }
);
    GenericUDF udf=new GenericUDFOPGreaterThan();
    List<ExprNodeDesc> children=Lists.newArrayList();
    ExprNodeColumnDesc columnDesc=new ExprNodeColumnDesc(Integer.class,"intCol","T",false);
    ExprNodeConstantDesc constantDesc=new ExprNodeConstantDesc(50);
    children.add(columnDesc);
    children.add(constantDesc);
    ExprNodeGenericFuncDesc genericFuncDesc=new ExprNodeGenericFuncDesc(inspector,udf,children);
    String searchArgumentStr=SerializationUtilities.serializeExpression(genericFuncDesc);
    conf.set(TableScanDesc.FILTER_EXPR_CONF_STR,searchArgumentStr);
    ParquetRecordReaderWrapper recordReader=(ParquetRecordReaderWrapper)new MapredParquetInputFormat().getRecordReader(new FileSplit(testPath,0,fileLength(testPath),(String[])null),conf,null);
    Assert.assertEquals("row group is not filtered correctly",1,recordReader.getFiltedBlocks().size());
    constantDesc=new ExprNodeConstantDesc(100);
    children.set(1,constantDesc);
    genericFuncDesc=new ExprNodeGenericFuncDesc(inspector,udf,children);
    searchArgumentStr=SerializationUtilities.serializeExpression(genericFuncDesc);
    conf.set(TableScanDesc.FILTER_EXPR_CONF_STR,searchArgumentStr);
    recordReader=(ParquetRecordReaderWrapper)new MapredParquetInputFormat().getRecordReader(new FileSplit(testPath,0,fileLength(testPath),(String[])null),conf,null);
    Assert.assertEquals("row group is not filtered correctly",0,recordReader.getFiltedBlocks().size());
  }
  private ArrayWritableObjectInspector getObjectInspector(  final String columnNames,  final String columnTypes){
    List<TypeInfo> columnTypeList=createHiveTypeInfoFrom(columnTypes);
    List<String> columnNameList=createHiveColumnsFrom(columnNames);
    StructTypeInfo rowTypeInfo=(StructTypeInfo)TypeInfoFactory.getStructTypeInfo(columnNameList,columnTypeList);
    return new ArrayWritableObjectInspector(rowTypeInfo);
  }
  private List<String> createHiveColumnsFrom(  final String columnNamesStr){
    List<String> columnNames;
    if (columnNamesStr.length() == 0) {
      columnNames=new ArrayList<String>();
    }
 else {
      columnNames=Arrays.asList(columnNamesStr.split(","));
    }
    return columnNames;
  }
  private List<TypeInfo> createHiveTypeInfoFrom(  final String columnsTypeStr){
    List<TypeInfo> columnTypes;
    if (columnsTypeStr.length() == 0) {
      columnTypes=new ArrayList<TypeInfo>();
    }
 else {
      columnTypes=TypeInfoUtils.getTypeInfosFromTypeString(columnsTypeStr);
    }
    return columnTypes;
  }
}
