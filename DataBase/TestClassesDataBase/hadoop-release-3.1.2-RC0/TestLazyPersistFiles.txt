public class TestLazyPersistFiles extends LazyPersistTestCase {
  private static final int THREADPOOL_SIZE=10;
  /** 
 * Append to lazy persist file is denied.
 * @throws IOException
 */
  @Test public void testAppendIsDenied() throws IOException {
    getClusterBuilder().build();
    final String METHOD_NAME=GenericTestUtils.getMethodName();
    Path path=new Path("/" + METHOD_NAME + ".dat");
    makeTestFile(path,BLOCK_SIZE,true);
    try {
      client.append(path.toString(),BUFFER_LENGTH,EnumSet.of(CreateFlag.APPEND),null,null).close();
      fail("Append to LazyPersist file did not fail as expected");
    }
 catch (    Throwable t) {
      LOG.info("Got expected exception ",t);
    }
  }
  /** 
 * Truncate to lazy persist file is denied.
 * @throws IOException
 */
  @Test public void testTruncateIsDenied() throws IOException {
    getClusterBuilder().build();
    final String METHOD_NAME=GenericTestUtils.getMethodName();
    Path path=new Path("/" + METHOD_NAME + ".dat");
    makeTestFile(path,BLOCK_SIZE,true);
    try {
      client.truncate(path.toString(),BLOCK_SIZE / 2);
      fail("Truncate to LazyPersist file did not fail as expected");
    }
 catch (    Throwable t) {
      LOG.info("Got expected exception ",t);
    }
  }
  /** 
 * If one or more replicas of a lazyPersist file are lost, then the file must be discarded by the NN, instead of being kept around as a 'corrupt' file.
 */
  @Test public void testCorruptFilesAreDiscarded() throws IOException, InterruptedException, TimeoutException {
    getClusterBuilder().setRamDiskReplicaCapacity(2).build();
    final String METHOD_NAME=GenericTestUtils.getMethodName();
    Path path1=new Path("/" + METHOD_NAME + ".01.dat");
    makeTestFile(path1,BLOCK_SIZE,true);
    ensureFileReplicasOnStorageType(path1,RAM_DISK);
    cluster.shutdownDataNodes();
    Thread.sleep(30000L);
    assertThat(cluster.getNamesystem().getNumDeadDataNodes(),is(1));
    Thread.sleep(2 * DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT * 1000);
    Thread.sleep(2 * LAZY_WRITE_FILE_SCRUBBER_INTERVAL_SEC * 1000);
    assert (!fs.exists(path1));
    assertThat(cluster.getNameNode().getNamesystem().getBlockManager().getLowRedundancyBlocksCount(),is(0L));
  }
  @Test public void testDisableLazyPersistFileScrubber() throws IOException, InterruptedException, TimeoutException {
    getClusterBuilder().setRamDiskReplicaCapacity(2).disableScrubber().build();
    final String METHOD_NAME=GenericTestUtils.getMethodName();
    Path path1=new Path("/" + METHOD_NAME + ".01.dat");
    makeTestFile(path1,BLOCK_SIZE,true);
    ensureFileReplicasOnStorageType(path1,RAM_DISK);
    cluster.shutdownDataNodes();
    Thread.sleep(30000L);
    Thread.sleep(2 * DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT * 1000);
    Thread.sleep(2 * LAZY_WRITE_FILE_SCRUBBER_INTERVAL_SEC * 1000);
    Assert.assertTrue(fs.exists(path1));
  }
  /** 
 * If NN restarted then lazyPersist files should not deleted
 */
  @Test(timeout=20000) public void testFileShouldNotDiscardedIfNNRestarted() throws IOException, InterruptedException, TimeoutException {
    getClusterBuilder().setRamDiskReplicaCapacity(2).build();
    final String METHOD_NAME=GenericTestUtils.getMethodName();
    Path path1=new Path("/" + METHOD_NAME + ".01.dat");
    makeTestFile(path1,BLOCK_SIZE,true);
    ensureFileReplicasOnStorageType(path1,RAM_DISK);
    cluster.shutdownDataNodes();
    cluster.restartNameNodes();
    Long corruptBlkCount;
    do {
      Thread.sleep(DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT * 1000);
      corruptBlkCount=(long)Iterators.size(cluster.getNameNode().getNamesystem().getBlockManager().getCorruptReplicaBlockIterator());
    }
 while (corruptBlkCount != 1L);
    Assert.assertTrue(fs.exists(path1));
  }
  /** 
 * Concurrent read from the same node and verify the contents.
 */
  @Test public void testConcurrentRead() throws Exception {
    getClusterBuilder().setRamDiskReplicaCapacity(2).build();
    final String METHOD_NAME=GenericTestUtils.getMethodName();
    final Path path1=new Path("/" + METHOD_NAME + ".dat");
    final int SEED=0xFADED;
    final int NUM_TASKS=5;
    makeRandomTestFile(path1,BLOCK_SIZE,true,SEED);
    ensureFileReplicasOnStorageType(path1,RAM_DISK);
    final CountDownLatch latch=new CountDownLatch(NUM_TASKS);
    final AtomicBoolean testFailed=new AtomicBoolean(false);
    Runnable readerRunnable=new Runnable(){
      @Override public void run(){
        try {
          Assert.assertTrue(verifyReadRandomFile(path1,BLOCK_SIZE,SEED));
        }
 catch (        Throwable e) {
          LOG.error("readerRunnable error",e);
          testFailed.set(true);
        }
 finally {
          latch.countDown();
        }
      }
    }
;
    Thread threads[]=new Thread[NUM_TASKS];
    for (int i=0; i < NUM_TASKS; i++) {
      threads[i]=new Thread(readerRunnable);
      threads[i].start();
    }
    Thread.sleep(500);
    for (int i=0; i < NUM_TASKS; i++) {
      Uninterruptibles.joinUninterruptibly(threads[i]);
    }
    Assert.assertFalse(testFailed.get());
  }
  /** 
 * Concurrent write with eviction RAM_DISK can hold 9 replicas 4 threads each write 5 replicas
 * @throws IOException
 * @throws InterruptedException
 */
  @Test public void testConcurrentWrites() throws IOException, InterruptedException {
    getClusterBuilder().setRamDiskReplicaCapacity(9).build();
    final String METHOD_NAME=GenericTestUtils.getMethodName();
    final int SEED=0xFADED;
    final int NUM_WRITERS=4;
    final int NUM_WRITER_PATHS=5;
    Path paths[][]=new Path[NUM_WRITERS][NUM_WRITER_PATHS];
    for (int i=0; i < NUM_WRITERS; i++) {
      paths[i]=new Path[NUM_WRITER_PATHS];
      for (int j=0; j < NUM_WRITER_PATHS; j++) {
        paths[i][j]=new Path("/" + METHOD_NAME + ".Writer"+ i+ ".File."+ j+ ".dat");
      }
    }
    final CountDownLatch latch=new CountDownLatch(NUM_WRITERS);
    final AtomicBoolean testFailed=new AtomicBoolean(false);
    ExecutorService executor=Executors.newFixedThreadPool(THREADPOOL_SIZE);
    for (int i=0; i < NUM_WRITERS; i++) {
      Runnable writer=new WriterRunnable(i,paths[i],SEED,latch,testFailed);
      executor.execute(writer);
    }
    Thread.sleep(3 * LAZY_WRITER_INTERVAL_SEC * 1000);
    triggerBlockReport();
    latch.await();
    assertThat(testFailed.get(),is(false));
  }
class WriterRunnable implements Runnable {
    private final int id;
    private final Path paths[];
    private final int seed;
    private CountDownLatch latch;
    private AtomicBoolean bFail;
    public WriterRunnable(    int threadIndex,    Path[] paths,    int seed,    CountDownLatch latch,    AtomicBoolean bFail){
      id=threadIndex;
      this.paths=paths;
      this.seed=seed;
      this.latch=latch;
      this.bFail=bFail;
      System.out.println("Creating Writer: " + id);
    }
    public void run(){
      System.out.println("Writer " + id + " starting... ");
      int i=0;
      try {
        for (i=0; i < paths.length; i++) {
          makeRandomTestFile(paths[i],BLOCK_SIZE,true,seed);
        }
      }
 catch (      IOException e) {
        bFail.set(true);
        LOG.error("Writer exception: writer id:" + id + " testfile: "+ paths[i].toString()+ " "+ e);
      }
 finally {
        latch.countDown();
      }
    }
  }
}
