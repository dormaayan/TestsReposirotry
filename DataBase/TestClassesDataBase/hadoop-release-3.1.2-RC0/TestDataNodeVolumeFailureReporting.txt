/** 
 * Test reporting of DN volume failure counts and metrics.
 */
public class TestDataNodeVolumeFailureReporting {
  private static final Log LOG=LogFactory.getLog(TestDataNodeVolumeFailureReporting.class);
{
    GenericTestUtils.setLogLevel(TestDataNodeVolumeFailureReporting.LOG,Level.ALL);
  }
  private FileSystem fs;
  private MiniDFSCluster cluster;
  private Configuration conf;
  private long volumeCapacity;
  final int WAIT_FOR_HEARTBEATS=3000;
  final int WAIT_FOR_DEATH=15000;
  @Rule public Timeout timeout=new Timeout(120 * 1000);
  @Before public void setUp() throws Exception {
    assumeNotWindows();
    initCluster(1,2,1);
  }
  @After public void tearDown() throws Exception {
    IOUtils.cleanup(LOG,fs);
    if (cluster != null) {
      cluster.shutdown();
      cluster=null;
    }
  }
  /** 
 * Test that individual volume failures do not cause DNs to fail, that all volumes failed on a single datanode do cause it to fail, and that the capacities and liveliness is adjusted correctly in the NN.
 */
  @Test public void testSuccessiveVolumeFailures() throws Exception {
    cluster.startDataNodes(conf,2,true,null,null);
    cluster.waitActive();
    Thread.sleep(WAIT_FOR_HEARTBEATS);
    final DatanodeManager dm=cluster.getNamesystem().getBlockManager().getDatanodeManager();
    final long origCapacity=DFSTestUtil.getLiveDatanodeCapacity(dm);
    long dnCapacity=DFSTestUtil.getDatanodeCapacity(dm,0);
    File dn1Vol1=cluster.getInstanceStorageDir(0,0);
    File dn2Vol1=cluster.getInstanceStorageDir(1,0);
    File dn3Vol1=cluster.getInstanceStorageDir(2,0);
    File dn3Vol2=cluster.getInstanceStorageDir(2,1);
    DataNodeTestUtils.injectDataDirFailure(dn1Vol1,dn2Vol1);
    Path file1=new Path("/test1");
    DFSTestUtil.createFile(fs,file1,1024,(short)3,1L);
    DFSTestUtil.waitReplication(fs,file1,(short)3);
    ArrayList<DataNode> dns=cluster.getDataNodes();
    assertTrue("DN1 should be up",dns.get(0).isDatanodeUp());
    assertTrue("DN2 should be up",dns.get(1).isDatanodeUp());
    assertTrue("DN3 should be up",dns.get(2).isDatanodeUp());
    checkFailuresAtDataNode(dns.get(0),1,true,dn1Vol1.getAbsolutePath());
    checkFailuresAtDataNode(dns.get(1),1,true,dn2Vol1.getAbsolutePath());
    checkFailuresAtDataNode(dns.get(2),0,true);
    assert (WAIT_FOR_HEARTBEATS * 10) > WAIT_FOR_DEATH;
    DFSTestUtil.waitForDatanodeStatus(dm,3,0,2,origCapacity - (1 * dnCapacity),WAIT_FOR_HEARTBEATS);
    checkAggregateFailuresAtNameNode(true,2);
    checkFailuresAtNameNode(dm,dns.get(0),true,dn1Vol1.getAbsolutePath());
    checkFailuresAtNameNode(dm,dns.get(1),true,dn2Vol1.getAbsolutePath());
    checkFailuresAtNameNode(dm,dns.get(2),true);
    DataNodeTestUtils.injectDataDirFailure(dn3Vol1);
    Path file2=new Path("/test2");
    DFSTestUtil.createFile(fs,file2,1024,(short)3,1L);
    DFSTestUtil.waitReplication(fs,file2,(short)3);
    assertTrue("DN3 should still be up",dns.get(2).isDatanodeUp());
    checkFailuresAtDataNode(dns.get(2),1,true,dn3Vol1.getAbsolutePath());
    DataNodeTestUtils.triggerHeartbeat(dns.get(2));
    checkFailuresAtNameNode(dm,dns.get(2),true,dn3Vol1.getAbsolutePath());
    dnCapacity=DFSTestUtil.getDatanodeCapacity(dm,0);
    DFSTestUtil.waitForDatanodeStatus(dm,3,0,3,origCapacity - (3 * dnCapacity),WAIT_FOR_HEARTBEATS);
    checkAggregateFailuresAtNameNode(true,3);
    checkFailuresAtNameNode(dm,dns.get(0),true,dn1Vol1.getAbsolutePath());
    checkFailuresAtNameNode(dm,dns.get(1),true,dn2Vol1.getAbsolutePath());
    checkFailuresAtNameNode(dm,dns.get(2),true,dn3Vol1.getAbsolutePath());
    DataNodeTestUtils.injectDataDirFailure(dn3Vol2);
    Path file3=new Path("/test3");
    DFSTestUtil.createFile(fs,file3,1024,(short)3,1L);
    DFSTestUtil.waitReplication(fs,file3,(short)2);
    checkFailuresAtDataNode(dns.get(2),2,true,dn3Vol1.getAbsolutePath(),dn3Vol2.getAbsolutePath());
    DFSTestUtil.waitForDatanodeDeath(dns.get(2));
    DFSTestUtil.waitForDatanodeStatus(dm,2,1,2,origCapacity - (4 * dnCapacity),WAIT_FOR_HEARTBEATS);
    checkAggregateFailuresAtNameNode(true,2);
    checkFailuresAtNameNode(dm,dns.get(0),true,dn1Vol1.getAbsolutePath());
    checkFailuresAtNameNode(dm,dns.get(1),true,dn2Vol1.getAbsolutePath());
    DataNodeTestUtils.restoreDataDirFromFailure(dn1Vol1,dn2Vol1,dn3Vol1,dn3Vol2);
    cluster.restartDataNodes();
    cluster.waitActive();
    Path file4=new Path("/test4");
    DFSTestUtil.createFile(fs,file4,1024,(short)3,1L);
    DFSTestUtil.waitReplication(fs,file4,(short)3);
    DFSTestUtil.waitForDatanodeStatus(dm,3,0,0,origCapacity,WAIT_FOR_HEARTBEATS);
    checkAggregateFailuresAtNameNode(true,0);
    dns=cluster.getDataNodes();
    checkFailuresAtNameNode(dm,dns.get(0),true);
    checkFailuresAtNameNode(dm,dns.get(1),true);
    checkFailuresAtNameNode(dm,dns.get(2),true);
  }
  /** 
 * Test that the NN re-learns of volume failures after restart.
 */
  @Test public void testVolFailureStatsPreservedOnNNRestart() throws Exception {
    cluster.startDataNodes(conf,2,true,null,null);
    cluster.waitActive();
    final DatanodeManager dm=cluster.getNamesystem().getBlockManager().getDatanodeManager();
    long origCapacity=DFSTestUtil.getLiveDatanodeCapacity(dm);
    long dnCapacity=DFSTestUtil.getDatanodeCapacity(dm,0);
    File dn1Vol1=cluster.getInstanceStorageDir(0,0);
    File dn2Vol1=cluster.getInstanceStorageDir(1,0);
    DataNodeTestUtils.injectDataDirFailure(dn1Vol1,dn2Vol1);
    Path file1=new Path("/test1");
    DFSTestUtil.createFile(fs,file1,1024,(short)2,1L);
    DFSTestUtil.waitReplication(fs,file1,(short)2);
    ArrayList<DataNode> dns=cluster.getDataNodes();
    DFSTestUtil.waitForDatanodeStatus(dm,3,0,2,origCapacity - (1 * dnCapacity),WAIT_FOR_HEARTBEATS);
    checkAggregateFailuresAtNameNode(true,2);
    checkFailuresAtNameNode(dm,dns.get(0),true,dn1Vol1.getAbsolutePath());
    checkFailuresAtNameNode(dm,dns.get(1),true,dn2Vol1.getAbsolutePath());
    cluster.restartNameNode(0);
    cluster.waitActive();
    DFSTestUtil.waitForDatanodeStatus(dm,3,0,2,origCapacity - (1 * dnCapacity),WAIT_FOR_HEARTBEATS);
    checkAggregateFailuresAtNameNode(true,2);
    checkFailuresAtNameNode(dm,dns.get(0),true,dn1Vol1.getAbsolutePath());
    checkFailuresAtNameNode(dm,dns.get(1),true,dn2Vol1.getAbsolutePath());
  }
  @Test public void testMultipleVolFailuresOnNode() throws Exception {
    tearDown();
    initCluster(3,4,2);
    Thread.sleep(WAIT_FOR_HEARTBEATS);
    DatanodeManager dm=cluster.getNamesystem().getBlockManager().getDatanodeManager();
    long origCapacity=DFSTestUtil.getLiveDatanodeCapacity(dm);
    long dnCapacity=DFSTestUtil.getDatanodeCapacity(dm,0);
    File dn1Vol1=cluster.getInstanceStorageDir(0,0);
    File dn1Vol2=cluster.getInstanceStorageDir(0,1);
    File dn2Vol1=cluster.getInstanceStorageDir(1,0);
    File dn2Vol2=cluster.getInstanceStorageDir(1,1);
    DataNodeTestUtils.injectDataDirFailure(dn1Vol1,dn1Vol2,dn2Vol1,dn2Vol2);
    Path file1=new Path("/test1");
    DFSTestUtil.createFile(fs,file1,1024,(short)3,1L);
    DFSTestUtil.waitReplication(fs,file1,(short)3);
    Path file2=new Path("/test2");
    DFSTestUtil.createFile(fs,file2,1024,(short)3,1L);
    DFSTestUtil.waitReplication(fs,file2,(short)3);
    ArrayList<DataNode> dns=cluster.getDataNodes();
    assertTrue("DN1 should be up",dns.get(0).isDatanodeUp());
    assertTrue("DN2 should be up",dns.get(1).isDatanodeUp());
    assertTrue("DN3 should be up",dns.get(2).isDatanodeUp());
    checkFailuresAtDataNode(dns.get(0),1,true,dn1Vol1.getAbsolutePath(),dn1Vol2.getAbsolutePath());
    checkFailuresAtDataNode(dns.get(1),1,true,dn2Vol1.getAbsolutePath(),dn2Vol2.getAbsolutePath());
    checkFailuresAtDataNode(dns.get(2),0,true);
    assert (WAIT_FOR_HEARTBEATS * 10) > WAIT_FOR_DEATH;
    DFSTestUtil.waitForDatanodeStatus(dm,3,0,4,origCapacity - (1 * dnCapacity),WAIT_FOR_HEARTBEATS);
    checkAggregateFailuresAtNameNode(true,4);
    checkFailuresAtNameNode(dm,dns.get(0),true,dn1Vol1.getAbsolutePath(),dn1Vol2.getAbsolutePath());
    checkFailuresAtNameNode(dm,dns.get(1),true,dn2Vol1.getAbsolutePath(),dn2Vol2.getAbsolutePath());
    checkFailuresAtNameNode(dm,dns.get(2),true);
  }
  @Test public void testDataNodeReconfigureWithVolumeFailures() throws Exception {
    cluster.startDataNodes(conf,2,true,null,null);
    cluster.waitActive();
    final DatanodeManager dm=cluster.getNamesystem().getBlockManager().getDatanodeManager();
    long origCapacity=DFSTestUtil.getLiveDatanodeCapacity(dm);
    long dnCapacity=DFSTestUtil.getDatanodeCapacity(dm,0);
    File dn1Vol1=cluster.getInstanceStorageDir(0,0);
    File dn1Vol2=cluster.getInstanceStorageDir(0,1);
    File dn2Vol1=cluster.getInstanceStorageDir(1,0);
    File dn2Vol2=cluster.getInstanceStorageDir(1,1);
    DataNodeTestUtils.injectDataDirFailure(dn1Vol1);
    DataNodeTestUtils.injectDataDirFailure(dn2Vol1);
    Path file1=new Path("/test1");
    DFSTestUtil.createFile(fs,file1,1024,(short)2,1L);
    DFSTestUtil.waitReplication(fs,file1,(short)2);
    ArrayList<DataNode> dns=cluster.getDataNodes();
    assertTrue("DN1 should be up",dns.get(0).isDatanodeUp());
    assertTrue("DN2 should be up",dns.get(1).isDatanodeUp());
    assertTrue("DN3 should be up",dns.get(2).isDatanodeUp());
    checkFailuresAtDataNode(dns.get(0),1,true,dn1Vol1.getAbsolutePath());
    checkFailuresAtDataNode(dns.get(1),1,true,dn2Vol1.getAbsolutePath());
    checkFailuresAtDataNode(dns.get(2),0,true);
    assert (WAIT_FOR_HEARTBEATS * 10) > WAIT_FOR_DEATH;
    DFSTestUtil.waitForDatanodeStatus(dm,3,0,2,origCapacity - (1 * dnCapacity),WAIT_FOR_HEARTBEATS);
    checkAggregateFailuresAtNameNode(true,2);
    checkFailuresAtNameNode(dm,dns.get(0),true,dn1Vol1.getAbsolutePath());
    checkFailuresAtNameNode(dm,dns.get(1),true,dn2Vol1.getAbsolutePath());
    DataNodeTestUtils.reconfigureDataNode(dns.get(0),dn1Vol1,dn1Vol2);
    DataNodeTestUtils.reconfigureDataNode(dns.get(1),dn2Vol1,dn2Vol2);
    DataNodeTestUtils.triggerHeartbeat(dns.get(0));
    DataNodeTestUtils.triggerHeartbeat(dns.get(1));
    checkFailuresAtDataNode(dns.get(0),1,true,dn1Vol1.getAbsolutePath());
    checkFailuresAtDataNode(dns.get(1),1,true,dn2Vol1.getAbsolutePath());
    assert (WAIT_FOR_HEARTBEATS * 10) > WAIT_FOR_DEATH;
    DFSTestUtil.waitForDatanodeStatus(dm,3,0,2,origCapacity - (1 * dnCapacity),WAIT_FOR_HEARTBEATS);
    checkAggregateFailuresAtNameNode(true,2);
    checkFailuresAtNameNode(dm,dns.get(0),true,dn1Vol1.getAbsolutePath());
    checkFailuresAtNameNode(dm,dns.get(1),true,dn2Vol1.getAbsolutePath());
    DataNodeTestUtils.reconfigureDataNode(dns.get(0),dn1Vol1,dn1Vol2);
    DataNodeTestUtils.reconfigureDataNode(dns.get(1),dn2Vol1,dn2Vol2);
    DataNodeTestUtils.triggerHeartbeat(dns.get(0));
    DataNodeTestUtils.triggerHeartbeat(dns.get(1));
    checkFailuresAtDataNode(dns.get(0),1,true,dn1Vol1.getAbsolutePath());
    checkFailuresAtDataNode(dns.get(1),1,true,dn2Vol1.getAbsolutePath());
    assert (WAIT_FOR_HEARTBEATS * 10) > WAIT_FOR_DEATH;
    DFSTestUtil.waitForDatanodeStatus(dm,3,0,2,origCapacity - (1 * dnCapacity),WAIT_FOR_HEARTBEATS);
    checkAggregateFailuresAtNameNode(true,2);
    checkFailuresAtNameNode(dm,dns.get(0),true,dn1Vol1.getAbsolutePath());
    checkFailuresAtNameNode(dm,dns.get(1),true,dn2Vol1.getAbsolutePath());
    DataNodeTestUtils.restoreDataDirFromFailure(dn1Vol1,dn2Vol1);
    DataNodeTestUtils.reconfigureDataNode(dns.get(0),dn1Vol1,dn1Vol2);
    DataNodeTestUtils.reconfigureDataNode(dns.get(1),dn2Vol1,dn2Vol2);
    DataNodeTestUtils.triggerHeartbeat(dns.get(0));
    DataNodeTestUtils.triggerHeartbeat(dns.get(1));
    checkFailuresAtDataNode(dns.get(0),1,true);
    checkFailuresAtDataNode(dns.get(1),1,true);
    DFSTestUtil.waitForDatanodeStatus(dm,3,0,0,origCapacity,WAIT_FOR_HEARTBEATS);
    checkAggregateFailuresAtNameNode(true,0);
    checkFailuresAtNameNode(dm,dns.get(0),true);
    checkFailuresAtNameNode(dm,dns.get(1),true);
  }
  @Test public void testAutoFormatEmptyDirectory() throws Exception {
    File dn1Vol1=cluster.getStorageDir(0,0);
    File current=new File(dn1Vol1,"current");
    File currentVersion=new File(current,"VERSION");
    currentVersion.delete();
    assertTrue(cluster.restartDataNodes(true));
    cluster.waitActive();
    ArrayList<DataNode> dns=cluster.getDataNodes();
    DataNode dn=dns.get(0);
    assertFalse("DataNode should not reformat if VERSION is missing",currentVersion.exists());
    final String[] expectedFailedVolumes={dn1Vol1.getAbsolutePath()};
    DataNodeTestUtils.triggerHeartbeat(dn);
    FsDatasetSpi<?> fsd=dn.getFSDataset();
    assertEquals(expectedFailedVolumes.length,fsd.getNumFailedVolumes());
    assertArrayEquals(expectedFailedVolumes,convertToAbsolutePaths(fsd.getFailedStorageLocations()));
    checkFailuresAtDataNode(dn,0,false,expectedFailedVolumes);
    final DatanodeManager dm=cluster.getNamesystem().getBlockManager().getDatanodeManager();
    long dnCapacity=DFSTestUtil.getDatanodeCapacity(dm,0);
    DFSTestUtil.waitForDatanodeStatus(dm,1,0,1,(1 * dnCapacity),WAIT_FOR_HEARTBEATS);
    checkAggregateFailuresAtNameNode(false,1);
    checkFailuresAtNameNode(dm,dns.get(0),false,dn1Vol1.getAbsolutePath());
  }
  @Test public void testAutoFormatEmptyBlockPoolDirectory() throws Exception {
    DataNode dn=cluster.getDataNodes().get(0);
    String bpid=cluster.getNamesystem().getBlockPoolId();
    BlockPoolSliceStorage bps=dn.getStorage().getBPStorage(bpid);
    Storage.StorageDirectory dir=bps.getStorageDir(0);
    File current=dir.getCurrentDir();
    File currentVersion=new File(current,"VERSION");
    currentVersion.delete();
    assertTrue(cluster.restartDataNodes(true));
    cluster.waitActive();
    assertFalse("DataNode should not reformat if VERSION is missing",currentVersion.exists());
  }
  /** 
 * Verify DataNode NumFailedVolumes and FailedStorageLocations after hot swap out of failed volume.
 */
  @Test public void testHotSwapOutFailedVolumeAndReporting() throws Exception {
    final File dn0Vol1=cluster.getInstanceStorageDir(0,0);
    final File dn0Vol2=cluster.getInstanceStorageDir(0,1);
    final DataNode dn0=cluster.getDataNodes().get(0);
    final String oldDataDirs=dn0.getConf().get(DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY);
    MBeanServer mbs=ManagementFactory.getPlatformMBeanServer();
    ObjectName mxbeanName=new ObjectName("Hadoop:service=DataNode,name=FSDatasetState-" + dn0.getDatanodeUuid());
    int numFailedVolumes=(int)mbs.getAttribute(mxbeanName,"NumFailedVolumes");
    Assert.assertEquals(dn0.getFSDataset().getNumFailedVolumes(),numFailedVolumes);
    checkFailuresAtDataNode(dn0,0,false,new String[]{});
    DataNodeTestUtils.injectDataDirFailure(dn0Vol1);
    DataNodeTestUtils.waitForDiskError(dn0,DataNodeTestUtils.getVolume(dn0,dn0Vol1));
    numFailedVolumes=(int)mbs.getAttribute(mxbeanName,"NumFailedVolumes");
    Assert.assertEquals(1,numFailedVolumes);
    Assert.assertEquals(dn0.getFSDataset().getNumFailedVolumes(),numFailedVolumes);
    checkFailuresAtDataNode(dn0,1,true,new String[]{dn0Vol1.getAbsolutePath()});
    try {
      dn0.reconfigurePropertyImpl(DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY,oldDataDirs);
      fail("Reconfigure with failed disk should throw exception.");
    }
 catch (    ReconfigurationException e) {
      Assert.assertTrue("Reconfigure exception doesn't have expected path!",e.getCause().getMessage().contains(dn0Vol1.getAbsolutePath()));
    }
    numFailedVolumes=(int)mbs.getAttribute(mxbeanName,"NumFailedVolumes");
    Assert.assertEquals(1,numFailedVolumes);
    Assert.assertEquals(dn0.getFSDataset().getNumFailedVolumes(),numFailedVolumes);
    checkFailuresAtDataNode(dn0,1,true,new String[]{dn0Vol1.getAbsolutePath()});
    String dataDirs=dn0Vol2.getPath();
    dn0.reconfigurePropertyImpl(DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY,dataDirs);
    numFailedVolumes=(int)mbs.getAttribute(mxbeanName,"NumFailedVolumes");
    Assert.assertEquals(0,numFailedVolumes);
    Assert.assertEquals(dn0.getFSDataset().getNumFailedVolumes(),numFailedVolumes);
    checkFailuresAtDataNode(dn0,0,true,new String[]{});
    DataNodeTestUtils.restoreDataDirFromFailure(dn0Vol1);
    dn0.reconfigurePropertyImpl(DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY,oldDataDirs);
    numFailedVolumes=(int)mbs.getAttribute(mxbeanName,"NumFailedVolumes");
    Assert.assertEquals(0,numFailedVolumes);
    Assert.assertEquals(dn0.getFSDataset().getNumFailedVolumes(),numFailedVolumes);
    checkFailuresAtDataNode(dn0,0,true,new String[]{});
    DataNodeTestUtils.injectDataDirFailure(dn0Vol2);
    DataNodeTestUtils.waitForDiskError(dn0,DataNodeTestUtils.getVolume(dn0,dn0Vol2));
    numFailedVolumes=(int)mbs.getAttribute(mxbeanName,"NumFailedVolumes");
    Assert.assertEquals(1,numFailedVolumes);
    Assert.assertEquals(dn0.getFSDataset().getNumFailedVolumes(),numFailedVolumes);
    checkFailuresAtDataNode(dn0,1,true,new String[]{dn0Vol2.getAbsolutePath()});
    assertTrue(dn0.shouldRun());
  }
  /** 
 * Checks the NameNode for correct values of aggregate counters tracking failed volumes across all DataNodes.
 * @param expectCapacityKnown if true, then expect that the capacities of thevolumes were known before the failures, and therefore the lost capacity can be reported
 * @param expectedVolumeFailuresTotal expected number of failed volumes
 */
  private void checkAggregateFailuresAtNameNode(  boolean expectCapacityKnown,  int expectedVolumeFailuresTotal){
    FSNamesystem ns=cluster.getNamesystem();
    assertEquals(expectedVolumeFailuresTotal,ns.getVolumeFailuresTotal());
    long expectedCapacityLost=getExpectedCapacityLost(expectCapacityKnown,expectedVolumeFailuresTotal);
    assertEquals(expectedCapacityLost,ns.getEstimatedCapacityLostTotal());
  }
  /** 
 * Checks a DataNode for correct reporting of failed volumes.
 * @param dn DataNode to check
 * @param expectedVolumeFailuresCounter metric counter value forVolumeFailures.  The current implementation actually counts the number of failed disk checker cycles, which may be different from the length of expectedFailedVolumes if multiple disks fail in the same disk checker cycle
 * @param expectCapacityKnown if true, then expect that the capacities of thevolumes were known before the failures, and therefore the lost capacity can be reported
 * @param expectedFailedVolumes expected locations of failed volumes
 * @throws Exception if there is any failure
 */
  private void checkFailuresAtDataNode(  DataNode dn,  long expectedVolumeFailuresCounter,  boolean expectCapacityKnown,  String... expectedFailedVolumes) throws Exception {
    FsDatasetSpi<?> fsd=dn.getFSDataset();
    StringBuilder strBuilder=new StringBuilder();
    strBuilder.append("expectedFailedVolumes is ");
    for (    String expected : expectedFailedVolumes) {
      strBuilder.append(expected + ",");
    }
    strBuilder.append(" fsd.getFailedStorageLocations() is ");
    for (    String expected : fsd.getFailedStorageLocations()) {
      strBuilder.append(expected + ",");
    }
    LOG.info(strBuilder.toString());
    final long actualVolumeFailures=getLongCounter("VolumeFailures",getMetrics(dn.getMetrics().name()));
    assertTrue("Actual async detected volume failures should be greater or " + "equal than " + expectedFailedVolumes,actualVolumeFailures >= expectedVolumeFailuresCounter);
    assertEquals(expectedFailedVolumes.length,fsd.getNumFailedVolumes());
    assertArrayEquals(expectedFailedVolumes,convertToAbsolutePaths(fsd.getFailedStorageLocations()));
    if (expectedFailedVolumes.length > 0) {
      assertTrue(fsd.getLastVolumeFailureDate() > 0);
      long expectedCapacityLost=getExpectedCapacityLost(expectCapacityKnown,expectedFailedVolumes.length);
      assertEquals(expectedCapacityLost,fsd.getEstimatedCapacityLostTotal());
    }
 else {
      assertEquals(0,fsd.getLastVolumeFailureDate());
      assertEquals(0,fsd.getEstimatedCapacityLostTotal());
    }
  }
  /** 
 * Checks NameNode tracking of a particular DataNode for correct reporting of failed volumes.
 * @param dm DatanodeManager to check
 * @param dn DataNode to check
 * @param expectCapacityKnown if true, then expect that the capacities of thevolumes were known before the failures, and therefore the lost capacity can be reported
 * @param expectedFailedVolumes expected locations of failed volumes
 * @throws Exception if there is any failure
 */
  private void checkFailuresAtNameNode(  DatanodeManager dm,  DataNode dn,  boolean expectCapacityKnown,  String... expectedFailedVolumes) throws Exception {
    DatanodeDescriptor dd=cluster.getNamesystem().getBlockManager().getDatanodeManager().getDatanode(dn.getDatanodeId());
    assertEquals(expectedFailedVolumes.length,dd.getVolumeFailures());
    VolumeFailureSummary volumeFailureSummary=dd.getVolumeFailureSummary();
    if (expectedFailedVolumes.length > 0) {
      assertArrayEquals(expectedFailedVolumes,convertToAbsolutePaths(volumeFailureSummary.getFailedStorageLocations()));
      assertTrue(volumeFailureSummary.getLastVolumeFailureDate() > 0);
      long expectedCapacityLost=getExpectedCapacityLost(expectCapacityKnown,expectedFailedVolumes.length);
      assertEquals(expectedCapacityLost,volumeFailureSummary.getEstimatedCapacityLostTotal());
    }
 else {
      assertNull(volumeFailureSummary);
    }
  }
  /** 
 * Converts the provided paths to absolute file paths.
 * @param locations the array of paths
 * @return array of absolute paths
 */
  private String[] convertToAbsolutePaths(  String[] locations){
    if (locations == null || locations.length == 0) {
      return new String[0];
    }
    String[] absolutePaths=new String[locations.length];
    for (int count=0; count < locations.length; count++) {
      try {
        absolutePaths[count]=new File(new URI(locations[count])).getAbsolutePath();
      }
 catch (      URISyntaxException e) {
        absolutePaths[count]=locations[count];
      }
    }
    return absolutePaths;
  }
  /** 
 * Returns expected capacity lost for use in assertions.  The return value is dependent on whether or not it is expected that the volume capacities were known prior to the failures.
 * @param expectCapacityKnown if true, then expect that the capacities of thevolumes were known before the failures, and therefore the lost capacity can be reported
 * @param expectedVolumeFailuresTotal expected number of failed volumes
 * @return estimated capacity lost in bytes
 */
  private long getExpectedCapacityLost(  boolean expectCapacityKnown,  int expectedVolumeFailuresTotal){
    return expectCapacityKnown ? expectedVolumeFailuresTotal * volumeCapacity : 0;
  }
  /** 
 * Initializes the cluster.
 * @param numDataNodes number of datanodes
 * @param storagesPerDatanode number of storage locations on each datanode
 * @param failedVolumesTolerated number of acceptable volume failures
 * @throws Exception if there is any failure
 */
  private void initCluster(  int numDataNodes,  int storagesPerDatanode,  int failedVolumesTolerated) throws Exception {
    conf=new HdfsConfiguration();
    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,512L);
    conf.setInt(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,1);
    conf.setInt(DFSConfigKeys.DFS_DF_INTERVAL_KEY,1000);
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY,1000);
    conf.setInt(DFSConfigKeys.DFS_DATANODE_FAILED_VOLUMES_TOLERATED_KEY,failedVolumesTolerated);
    conf.setTimeDuration(DFSConfigKeys.DFS_DATANODE_DISK_CHECK_MIN_GAP_KEY,0,TimeUnit.MILLISECONDS);
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes).storagesPerDatanode(storagesPerDatanode).build();
    cluster.waitActive();
    fs=cluster.getFileSystem();
    long dnCapacity=DFSTestUtil.getDatanodeCapacity(cluster.getNamesystem().getBlockManager().getDatanodeManager(),0);
    volumeCapacity=dnCapacity / cluster.getStoragesPerDatanode();
  }
}
